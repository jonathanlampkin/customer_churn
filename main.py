"""
Customer Churn Prediction Dashboard - All-in-One Solution
Handles data processing, model training, and visualization in one file.
"""
import os
import sys
import time
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import streamlit as st
from scipy.io import arff
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import xgboost as xgb
import lightgbm as lgb
from sklearn.inspection import permutation_importance
import seaborn as sns
import warnings
import logging
import datetime
import os.path
import threading
import math
import plotly.graph_objects as go

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('churn_app.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

# Suppress warnings
warnings.filterwarnings('ignore')

# Create necessary directories
os.makedirs('reports/metrics', exist_ok=True)
os.makedirs('reports/figures', exist_ok=True)
os.makedirs('data/interim', exist_ok=True)

# Path to data - hardcoded now instead of using UI control
DEFAULT_DATA_PATH = "/mnt/hdd/Documents/mobile_churn_66kx66_numeric_nonull"

# File monitoring function
def monitor_metrics_file():
    metrics_file = 'reports/metrics/final_model_metrics.json'
    last_modified = os.path.getmtime(metrics_file) if os.path.exists(metrics_file) else 0
    
    while True:
        time.sleep(2)  # Check every 2 seconds
        if os.path.exists(metrics_file):
            current_modified = os.path.getmtime(metrics_file)
            if current_modified > last_modified:
                print(f"!!! ALERT: Metrics file was modified at {time.ctime(current_modified)} !!!")
                last_modified = current_modified

# Start monitoring in a background thread
if not os.environ.get('METRICS_MONITOR_RUNNING'):
    os.environ['METRICS_MONITOR_RUNNING'] = 'true'
    monitor_thread = threading.Thread(target=monitor_metrics_file, daemon=True)
    monitor_thread.start()

def read_logs(num_lines=50):
    """Read the most recent lines from the log file"""
    try:
        log_file = 'churn_app.log'
        if os.path.exists(log_file):
            with open(log_file, 'r') as f:
                lines = f.readlines()
                return lines[-num_lines:]  # Return last N lines
        return ["No log file found"]
    except Exception as e:
        return [f"Error reading logs: {str(e)}"]

def process_data(data_path=DEFAULT_DATA_PATH):
    """Process data and train models if results don't exist"""
    logger.info("============= STARTING DATA PROCESSING =============")
    logger.info(f"Processing data from: {data_path}")
    
    # Create timing info
    start_time = time.time()
    
    # FORCE REPROCESSING FOR TESTING - remove this in production
    metrics_file = 'reports/metrics/final_model_metrics.json'
    if os.path.exists(metrics_file):
        logger.info(f"Removing existing metrics file to force reprocessing")
        os.remove(metrics_file)
    
    # Now check if we already have results (this should always be false now)
    if os.path.exists(metrics_file):
        logger.info(f"Metrics file {metrics_file} already exists, skipping processing")
        return True
    
    # Create a placeholder for progress updates
    progress_container = st.empty()
    with progress_container.container():
        progress = st.progress(0)
        status = st.info("Starting data processing...")
    
    try:
        # Update status
        logger.info("Loading data from ARFF file...")
        status.info("Loading data...")
        
        # Load and parse the ARFF file
        data, meta = arff.loadarff(data_path)
        
        # Convert to pandas DataFrame
        df = pd.DataFrame(data)
        
        # Convert byte strings to regular strings
        for col in df.select_dtypes(include=['object']).columns:
            df[col] = df[col].str.decode('utf-8')
        
        logger.info(f"Data loaded: {df.shape[0]} rows, {df.shape[1]} columns")
        status.info(f"Data loaded: {df.shape[0]} rows, {df.shape[1]} columns")
        progress.progress(10)
        
        # Find target column
        target_column = None
        for col in df.columns:
            if 'churn' in col.lower():
                target_column = col
                break
        
        if not target_column:
            target_column = df.columns[-1]
            logger.info(f"No column with 'churn' in name found, using last column: {target_column}")
        else:
            logger.info(f"Using '{target_column}' as target column")
        
        # Remove user_account_id or any ID columns before modeling
        columns_to_remove = []
        id_patterns = ['id', 'account_id', 'user_id', 'customer_id', 'user_account_id']
        
        for col in df.columns:
            for pattern in id_patterns:
                if pattern.lower() in col.lower():
                    columns_to_remove.append(col)
                    logger.info(f"Removing ID column: {col}")
                    break
                    
        # Remove the ID columns
        if columns_to_remove:
            df = df.drop(columns=columns_to_remove)
            logger.info(f"Removed {len(columns_to_remove)} ID columns")
            status.info(f"Removed {len(columns_to_remove)} ID columns that are not predictive")
        
        # Prepare features and target
        X = df.drop(columns=[target_column])
        y = df[target_column]
        
        # Calculate and save churn rate (global dataset)
        churn_rate = y.mean()
        logger.info(f"Global churn rate: {churn_rate:.4f} ({churn_rate*100:.2f}%)")
        with open('reports/metrics/churn_rate.json', 'w') as f:
            json.dump({'churn_rate': float(churn_rate)}, f)
        
        # Split the data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        logger.info(f"Data split completed: {X_train.shape[0]} training rows, {X_test.shape[0]} test rows")
        status.info(f"Data split: {X_train.shape[0]} training rows, {X_test.shape[0]} test rows")
        progress.progress(20)
        
        # Save interim data
        logger.info("Saving interim data files...")
        X_train.to_csv('data/interim/X_train.csv', index=False)
        X_test.to_csv('data/interim/X_test.csv', index=False)
        y_train.to_csv('data/interim/y_train.csv', index=False)
        y_test.to_csv('data/interim/y_test.csv', index=False)
        
        # Initialize all the models
        logger.info("====== CREATING MODELS ======")
        try:
            # Base models
            logger.info("Creating base models...")
            
            # List to capture any model creation errors
            model_errors = []
            
            # Create all models with error handling
            base_models = []
            
            # Add Logistic Regression
            try:
                base_models.append(('logistic_regression', LogisticRegression(max_iter=1000, random_state=42)))
                logger.info("Created Logistic Regression model")
            except Exception as e:
                error_msg = f"Error creating Logistic Regression: {str(e)}"
                logger.error(error_msg)
                model_errors.append(error_msg)
            
            # Add Random Forest
            try:
                base_models.append(('random_forest', RandomForestClassifier(n_estimators=100, random_state=42)))
                logger.info("Created Random Forest model")
            except Exception as e:
                error_msg = f"Error creating Random Forest: {str(e)}"
                logger.error(error_msg)
                model_errors.append(error_msg)
            
            # Add Gradient Boosting
            try:
                base_models.append(('gradient_boosting', GradientBoostingClassifier(n_estimators=100, random_state=42)))
                logger.info("Created Gradient Boosting model")
            except Exception as e:
                error_msg = f"Error creating Gradient Boosting: {str(e)}"
                logger.error(error_msg)
                model_errors.append(error_msg)
            
            # Add XGBoost - handle if package is missing
            try:
                base_models.append(('xgboost', xgb.XGBClassifier(n_estimators=100, random_state=42)))
                logger.info("Created XGBoost model")
            except Exception as e:
                error_msg = f"Error creating XGBoost: {str(e)}"
                logger.error(error_msg)
                model_errors.append(error_msg)
            
            # Add LightGBM - handle if package is missing
            try:
                base_models.append(('lightgbm', lgb.LGBMClassifier(n_estimators=100, random_state=42)))
                logger.info("Created LightGBM model")
            except Exception as e:
                error_msg = f"Error creating LightGBM: {str(e)}"
                logger.error(error_msg)
                model_errors.append(error_msg)
            
            # Log any errors that occurred
            if model_errors:
                error_summary = "\n".join(model_errors)
                logger.warning(f"Encountered errors creating some models:\n{error_summary}")
                status.warning(f"Some models could not be created. Check logs for details.")
            
            # Create dictionary of models
            models = {name: model for name, model in base_models}
            
            model_names = list(models.keys())
            logger.info(f"Successfully created {len(models)} models: {', '.join(model_names)}")
            status.info(f"Created {len(models)} models")
            
        except Exception as e:
            logger.error(f"Error creating models: {str(e)}", exc_info=True)
            status.error(f"Error creating models: {e}")
            return False
        
        # Train and evaluate each model
        logger.info("====== TRAINING MODELS ======")
        all_metrics = {}
        best_auc = 0
        best_model_name = None
        
        # Capture all model info for logging
        model_results = {}
        
        for i, (name, model) in enumerate(models.items()):
            model_start_time = time.time()
            logger.info(f"Training model: {name}")
            progress_value = 20 + (i * 10)  # Distribute progress from 20% to 80%
            status.info(f"Training {name.replace('_', ' ').title()} model...")
            progress.progress(progress_value)
            
            try:
                # Train the model
                model.fit(X_train, y_train)
                logger.info(f"Fitted {name} model to training data")
                
                # Evaluate
                y_pred = model.predict(X_test)
                y_prob = model.predict_proba(X_test)[:, 1]
                
                # Calculate metrics
                metrics_dict = {
                    'accuracy': float(accuracy_score(y_test, y_pred)),
                    'precision': float(precision_score(y_test, y_pred, zero_division=0)),
                    'recall': float(recall_score(y_test, y_pred, zero_division=0)),
                    'f1': float(f1_score(y_test, y_pred, zero_division=0)),
                    'roc_auc': float(roc_auc_score(y_test, y_prob)),
                }
                
                all_metrics[name] = metrics_dict
                
                # Track model info for logging
                model_results[name] = {
                    'training_time': time.time() - model_start_time,
                    'metrics': metrics_dict
                }
                
                # Track the best model
                if metrics_dict['roc_auc'] > best_auc:
                    best_auc = metrics_dict['roc_auc']
                    best_model_name = name
                    
                logger.info(f"Trained {name}: Accuracy={metrics_dict['accuracy']:.4f}, AUC={metrics_dict['roc_auc']:.4f}")
                status.info(f"Trained {name.replace('_', ' ').title()}: AUC = {metrics_dict['roc_auc']:.3f}")
                
            except Exception as e:
                logger.error(f"Error training {name} model: {str(e)}", exc_info=True)
                status.error(f"Error training {name} model: {e}")
                all_metrics[name] = {
                    'error': str(e)
                }
                model_results[name] = {
                    'training_time': time.time() - model_start_time,
                    'error': str(e)
                }
        
        progress.progress(80)
        
        # Log model comparison table
        logger.info("====== MODEL COMPARISON ======")
        log_table = "Model Comparison:\n"
        log_table += "-" * 80 + "\n"
        log_table += f"{'Model':<20} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1':<10} {'ROC AUC':<10} {'Time(s)':<10}\n"
        log_table += "-" * 80 + "\n"
        
        for name, result in model_results.items():
            if 'error' not in result:
                metrics = result['metrics']
                log_table += f"{name:<20} {metrics['accuracy']:<10.4f} {metrics['precision']:<10.4f} "
                log_table += f"{metrics['recall']:<10.4f} {metrics['f1']:<10.4f} {metrics['roc_auc']:<10.4f} "
                log_table += f"{result['training_time']:<10.2f}\n"
            else:
                log_table += f"{name:<20} ERROR: {result['error']}\n"
        
        log_table += "-" * 80 + "\n"
        log_table += f"Best model: {best_model_name} (AUC = {best_auc:.4f})\n"
        
        logger.info("\n" + log_table)
        
        # Save all metrics
        metrics = {
            'best_model': best_model_name,
            'metrics': all_metrics,
            'processing_timestamp': datetime.datetime.now().isoformat()
        }
        
        # Create metrics directory if it doesn't exist
        os.makedirs('reports/metrics', exist_ok=True)
        with open('reports/metrics/final_model_metrics.json', 'w') as f:
            json.dump(metrics, f, indent=4)
        
        logger.info(f"Saved metrics to reports/metrics/final_model_metrics.json")
        status.info(f"Saved metrics to reports/metrics/final_model_metrics.json")
        
        # Create a churn distribution visualization
        logger.info("Creating visualizations...")
        if not os.path.exists('reports/figures'):
            os.makedirs('reports/figures', exist_ok=True)
            
        try:
            plt.figure(figsize=(10, 6))
            ax = sns.countplot(x=y)
            ax.set_title('Distribution of Churn')
            ax.set_xlabel('Churn')
            ax.set_ylabel('Count')
            plt.tight_layout()
            plt.savefig('reports/figures/churn_distribution.png')
            plt.close()
            logger.info("Created churn distribution plot")
        except Exception as e:
            logger.error(f"Error creating churn distribution plot: {str(e)}", exc_info=True)
            status.warning(f"Error creating churn distribution plot: {e}")
        
        # Get feature importance from the best model
        logger.info("====== GENERATING FEATURE IMPORTANCE ======")
        if best_model_name:
            best_model = models[best_model_name]
            logger.info(f"Calculating feature importance for {best_model_name}")
            
            # Get feature importance based on model type
            if hasattr(best_model, 'feature_importances_'):
                # Tree-based models
                logger.info("Using native feature_importances_ from model")
                importance = pd.DataFrame({
                    'Feature': X.columns,
                    'Importance': best_model.feature_importances_
                }).sort_values('Importance', ascending=False)
            elif hasattr(best_model, 'coef_'):
                # Linear models
                logger.info("Using coefficients from linear model")
                importance = pd.DataFrame({
                    'Feature': X.columns,
                    'Importance': np.abs(best_model.coef_[0])
                }).sort_values('Importance', ascending=False)
            else:
                # Fallback: If the model doesn't provide importance, use permutation importance
                logger.info("Using permutation importance as fallback")
                perm_importance = permutation_importance(best_model, X_test, y_test, n_repeats=5, random_state=42)
                importance = pd.DataFrame({
                    'Feature': X.columns,
                    'Importance': perm_importance.importances_mean
                }).sort_values('Importance', ascending=False)
            
            # Save feature importance
            importance.to_csv('reports/metrics/feature_importance.csv', index=False)
            logger.info(f"Saved feature importance to reports/metrics/feature_importance.csv")
            logger.info(f"Top 10 features: {', '.join(importance.head(10)['Feature'].tolist())}")
            status.info("Saved feature importance to reports/metrics/feature_importance.csv")
        else:
            logger.error("No best model identified. Cannot generate feature importance.")
            status.error("No best model identified. Cannot generate feature importance.")
        
        # Generate sample explanations
        logger.info("====== GENERATING SAMPLE EXPLANATIONS ======")
        status.info("Generating sample explanations...")
        progress.progress(90)
        
        # Create sample explanations based on real test examples
        explanations = []
        np.random.seed(42)  # For reproducibility
        sample_indices = np.random.choice(X_test.shape[0], 5, replace=False)
        
        if best_model_name:
            best_model = models[best_model_name]
            logger.info(f"Creating explanations for {len(sample_indices)} test samples using {best_model_name}")
            
            for idx in sample_indices:
                sample = X_test.iloc[idx:idx+1]
                actual = y_test.iloc[idx]
                pred_prob = best_model.predict_proba(sample)[0, 1]
                
                # Get top features
                top_features = importance.head(5)['Feature'].values
                
                # Create feature contributions
                feature_contributions = []
                for feature in top_features:
                    if feature in X.columns:  # Make sure feature exists in X (handle stacking features)
                        feature_val = sample[feature].values[0]
                        
                        # Determine contribution direction using permutation
                        orig_prob = best_model.predict_proba(sample)[0, 1]
                        
                        # Create modified sample with mean value for this feature
                        mod_sample = sample.copy()
                        mod_sample[feature] = X[feature].mean()
                        mod_prob = best_model.predict_proba(mod_sample)[0, 1]
                        
                        # Direction of impact
                        contribution = orig_prob - mod_prob
                        
                        feature_contributions.append({
                            "feature": feature,
                            "value": float(feature_val) if isinstance(feature_val, (int, float)) else str(feature_val),
                            "contribution": float(contribution)
                        })
                
                # Create explanation object
                explanations.append({
                    "sample_id": int(idx),
                    "prediction": float(pred_prob),
                    "actual": float(actual),
                    "feature_contributions": feature_contributions
                })
                
                logger.info(f"Created explanation for sample {idx}: pred={pred_prob:.4f}, actual={actual}")
            
            # Save explanations
            with open('reports/metrics/sample_explanations.json', 'w') as f:
                json.dump(explanations, f, indent=4)
            
            logger.info(f"Saved {len(explanations)} sample explanations to reports/metrics/sample_explanations.json")
            status.info("Saved sample explanations to reports/metrics/sample_explanations.json")
        else:
            logger.error("No best model identified. Cannot generate explanations.")
            status.error("No best model identified. Cannot generate explanations.")
        
        # Clean up 
        progress.progress(100)
        status.success("Data processing complete!")
        progress_container.empty()
        
        # Log final timing
        total_time = time.time() - start_time
        logger.info(f"============= DATA PROCESSING COMPLETED in {total_time:.2f} seconds =============")
        
        # Generate EDA visualizations
        generate_eda_visualizations(df, target_column)
        
        return True
        
    except Exception as e:
        logger.error(f"Error processing data: {str(e)}", exc_info=True)
        status.error(f"Error processing data: {e}")
        import traceback
        status.error(traceback.format_exc())
        return False

def get_churn_rate():
    """Get the churn rate either from the metrics file or calculate it directly"""
    try:
        # First try to load from the saved metrics file
        churn_rate_file = 'reports/metrics/churn_rate.json'
        if os.path.exists(churn_rate_file):
            with open(churn_rate_file, 'r') as f:
                data = json.load(f)
                return data.get('churn_rate', None)
        
        # If file doesn't exist, try to calculate from the original data
        data, meta = arff.loadarff(DEFAULT_DATA_PATH)
        df = pd.DataFrame(data)
        
        # Find target column (churn)
        target_col = None
        for col in df.columns:
            if 'churn' in col.lower():
                target_col = col
                break
        
        if target_col:
            # Convert target to numeric if needed
            if df[target_col].dtype == object:
                # Handle byte strings if present
                if isinstance(df[target_col].iloc[0], bytes):
                    df[target_col] = df[target_col].str.decode('utf-8')
                # Convert yes/no, true/false to 1/0
                if df[target_col].dtype == object:
                    df[target_col] = df[target_col].map(lambda x: 1 if x in ['1', 'yes', 'true', 'Yes', 'True'] else 0)
            
            # Calculate churn rate
            churn_rate = df[target_col].mean()
            logger.info(f"Calculated churn rate directly: {churn_rate:.4f}")
            return churn_rate
        
        return None
    except Exception as e:
        logger.error(f"Error getting churn rate: {str(e)}")
        return None

def load_metrics():
    """Load metrics from file"""
    try:
        metrics_file = 'reports/metrics/final_model_metrics.json'
        if os.path.exists(metrics_file):
            with open(metrics_file, 'r') as f:
                metrics_data = json.load(f)
                logger.info(f"Loaded metrics with models: {list(metrics_data.get('metrics', {}).keys())}")
                return metrics_data
        logger.warning("Metrics file does not exist")
        return None
    except Exception as e:
        logger.error(f"Error loading metrics: {str(e)}", exc_info=True)
        st.error(f"Error loading metrics: {e}")
        return None

def load_feature_importance():
    """Load feature importance from file"""
    try:
        importance_file = 'reports/metrics/feature_importance.csv'
        if os.path.exists(importance_file):
            return pd.read_csv(importance_file)
        logger.warning("Feature importance file does not exist")
        return None
    except Exception as e:
        logger.error(f"Error loading feature importance: {str(e)}", exc_info=True)
        st.error(f"Error loading feature importance: {e}")
        return None

def load_sample_explanations():
    """Load sample explanations from file"""
    try:
        explanations_file = 'reports/metrics/sample_explanations.json'
        if os.path.exists(explanations_file):
            with open(explanations_file, 'r') as f:
                return json.load(f)
        logger.warning("Sample explanations file does not exist")
        return None
    except Exception as e:
        logger.error(f"Error loading explanations: {str(e)}", exc_info=True)
        st.error(f"Error loading explanations: {e}")
        return None

# Page configuration MUST come first
st.set_page_config(
    page_title="Customer Churn Prediction",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# IMPORTANT: Load the data first before using it
# Load data ONCE at app startup
metrics = load_metrics()
importance_df = load_feature_importance()
explanations = load_sample_explanations()

# Auto-process data if metrics don't exist
if metrics is None:
    logger.info("No metrics found - automatically starting data processing")
    with st.spinner("Initializing data processing... This may take a few minutes"):
        process_data(DEFAULT_DATA_PATH)
        # Reload metrics after processing
        metrics = load_metrics()
        importance_df = load_feature_importance()
        explanations = load_sample_explanations()

# Main Streamlit app

# Sidebar
st.sidebar.title("Customer Churn Analysis")

# Navigation
pages = ["Overview", "Exploratory Data Analysis", "Model Performance", "Feature Importance", "Prediction Explanations", "System Logs"]
page = st.sidebar.radio("Navigate", pages)

# Pages
if page == "Overview":
    # Use custom styling for the header
    st.markdown('<h1 style="color: #1E88E5;">Customer Churn Prediction</h1>', unsafe_allow_html=True)
    
    # Project description
    st.markdown("""
    This dashboard provides insights from a machine learning project focused on predicting customer churn.
    The analysis helps identify factors that contribute to customer attrition and enables proactive retention strategies.
    """)
    
    # Key Highlights
    st.markdown("### Key Highlights")
    st.markdown("""
    * **Objective**: Predict which customers are likely to churn and identify key risk factors
    * **Approach**: Applied multiple machine learning algorithms to historical customer data
    * **Outcome**: Deployed a model that can identify at-risk customers with high accuracy
    * **Impact**: Enables targeted retention efforts to reduce customer attrition
    """)
    
    # Business Impact section (keep this)
    st.markdown("### Business Impact")
    
    # Get the churn rate
    churn_rate = get_churn_rate()
    
    # Calculate revenue loss from the dataset
    try:
        # Try to load the original data
        data, meta = arff.loadarff(DEFAULT_DATA_PATH)
        df = pd.DataFrame(data)
        
        # Convert byte strings to regular strings
        for col in df.select_dtypes(include=['object']).columns:
            df[col] = df[col].str.decode('utf-8')
        
        # Find target column and monthly charges
        target_column = None
        monthly_charges_col = None
        
        # Identify the churn column
        for col in df.columns:
            if 'churn' in col.lower():
                target_column = col
                break
        
        # Identify the monthly charges column
        for col in df.columns:
            if 'monthly' in col.lower() and 'charge' in col.lower():
                monthly_charges_col = col
                break
        
        # If we have both columns, calculate revenue loss
        if target_column and monthly_charges_col:
            # Get customers who churned
            churned = df[df[target_column] == 1]
            
            # Calculate total monthly charges for churned customers
            monthly_charges_sum = churned[monthly_charges_col].sum()
            
            # Calculate annual loss
            annual_loss = monthly_charges_sum * 12
            
            # Format the number
            if annual_loss >= 1e6:
                revenue_loss = f"${annual_loss/1e6:.2f}M"
            elif annual_loss >= 1e3:
                revenue_loss = f"${annual_loss/1e3:.0f}K"
            else:
                revenue_loss = f"${annual_loss:.0f}"
                
            logger.info(f"Calculated annual revenue loss: {revenue_loss}")
            
            # Display metrics in columns
            col1, col2 = st.columns(2)
            
            if churn_rate is not None:
                col1.metric("Churn Rate", f"{churn_rate:.1%}")
            else:
                col1.metric("Churn Rate", "Not available")
            
            col2.metric("Annual Revenue Loss", revenue_loss)
        else:
            # If we couldn't find the right columns, show what we can
            col1, col2 = st.columns(2)
            if churn_rate is not None:
                col1.metric("Churn Rate", f"{churn_rate:.1%}")
            else:
                col1.metric("Churn Rate", "Not available")
            col2.metric("Annual Revenue Loss", "Could not calculate")
            
    except Exception as e:
        logger.error(f"Error calculating revenue loss: {str(e)}", exc_info=True)
        
        col1, col2 = st.columns(2)
        if churn_rate is not None:
            col1.metric("Churn Rate", f"{churn_rate:.1%}")
        else:
            col1.metric("Churn Rate", "Not available")
        col2.metric("Annual Revenue Loss", "Error: See logs")
    
    # Project Workflow table
    st.markdown("### Project Workflow")
    
    workflow_data = {
        'Phase': [
            '1', '2', '3', '4', '5', '6'
        ],
        'Description': [
            'Data Collection & Cleaning', 
            'Exploratory Data Analysis', 
            'Feature Engineering', 
            'Model Development', 
            'Hyperparameter Tuning', 
            'Model Evaluation & Deployment'
        ],
        'Status': ['Complete', 'Complete', 'Complete', 'Complete', 'Complete', 'Complete'],
    }
    
    workflow_df = pd.DataFrame(workflow_data)
    st.table(workflow_df)
    
    # Technical Stack
    st.markdown("### Technical Stack")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("**Data Processing**")
        st.markdown("""
        - Polars / Pandas
        - NumPy
        - SciPy
        """)
    
    with col2:
        st.markdown("**ML & Modeling**")
        st.markdown("""
        - Scikit-learn
        - XGBoost
        - LightGBM
        """)
    
    with col3:
        st.markdown("**Visualization**")
        st.markdown("""
        - Matplotlib
        - Plotly
        - Streamlit
        """)

elif page == "Exploratory Data Analysis":
    st.title("Exploratory Data Analysis")
    
    # Add CSS to make the tab content area look better
    st.markdown("""
    <style>
    .stTabs [data-baseweb="tab-panel"] {
        background-color: #f5f7fa;
        padding: 20px;
        border-radius: 8px;
        box-shadow: 0 1px 2px rgba(0,0,0,0.1);
    }
    .stTabs [data-baseweb="tab"] {
        font-size: 16px;
        font-weight: 600;
    }
    .plot-container {
        background-color: white;
        padding: 15px;
        border-radius: 8px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        margin-bottom: 20px;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Add description
    st.markdown("""
    This section presents the exploratory analysis performed on the customer churn dataset.
    We investigate data distributions, relationships between features, and characteristics of customers who churn.
    """)
    
    # Create tabs for different visualizations
    eda_tabs = st.tabs([
        "Class Distribution", 
        "Feature Histograms", 
        "Correlation Matrix", 
        "Feature Analysis"
    ])
    
    with eda_tabs[0]:  # Class Distribution
        # Get churn rate for display
        churn_rate = get_churn_rate()
        if churn_rate is not None:
            # Create a pie chart of class distribution with Plotly
            st.markdown('<div class="plot-container">', unsafe_allow_html=True)
            col1, col2 = st.columns([2, 3])
            
            with col1:
                labels = ['Retained', 'Churned']
                values = [1-churn_rate, churn_rate]
                
                fig = go.Figure(data=[go.Pie(
                    labels=labels,
                    values=values,
                    hole=.4,
                    marker_colors=['#1E88E5', '#FFC107'],
                    textinfo='percent',
                    texttemplate='%{percent:.1%}',
                    textposition='inside',
                    textfont_size=16,
                    pull=[0, 0.1],
                    hoverinfo='label+percent'
                )])
                
                fig.update_layout(
                    title_text="Customer Churn Distribution",
                    title_font_size=20,
                    title_x=0.5,
                    annotations=[dict(
                        text=f'Churn<br>Rate<br>{churn_rate:.1%}',
                        x=0.5, y=0.5,
                        font_size=16,
                        showarrow=False
                    )],
                    showlegend=True,
                    legend=dict(
                        orientation="h",
                        yanchor="bottom",
                        y=-0.2,
                        xanchor="center",
                        x=0.5
                    ),
                    margin=dict(t=60, b=0, l=0, r=0),
                    height=400
                )
                
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.markdown(f"""
                ### Churn Rate Analysis
                
                * **Overall Churn Rate**: {churn_rate:.1%}
                * **Retained Customers**: {100-churn_rate*100:.1f}%
                * **Churned Customers**: {churn_rate*100:.1f}%
                
                **Business Impact**: At this churn rate, for every 1,000 customers, approximately {int(churn_rate*1000)} are lost.
                
                **Class Imbalance**: The dataset shows an imbalance with only {churn_rate:.1%} of customers churning.
                This imbalance was addressed during model training using:
                - Class weighting
                - Evaluation metrics suited for imbalanced data (ROC-AUC, PR-AUC)
                - Threshold optimization
                """)
            st.markdown('</div>', unsafe_allow_html=True)
        else:
            st.warning("Churn rate data not available. Please process the data first.")
    
    with eda_tabs[1]:  # Feature Histograms
        st.subheader("Distribution of Key Features")
        
        st.markdown('<div class="plot-container">', unsafe_allow_html=True)
        if os.path.exists('reports/figures/feature_distributions.png'):
            st.image('reports/figures/feature_distributions.png')
            st.caption("Distributions show how feature values differ between churned and retained customers")
        else:
            st.info("Feature distribution visualizations will appear after processing data. Please run the data processing to generate this visualization.")
            if st.button("Generate Feature Distributions"):
                process_data(DEFAULT_DATA_PATH)
                st.experimental_rerun()
        st.markdown('</div>', unsafe_allow_html=True)
    
    with eda_tabs[2]:  # Correlation Matrix
        st.subheader("Feature Correlation Analysis")
        
        st.markdown('<div class="plot-container">', unsafe_allow_html=True)
        if os.path.exists('reports/figures/correlation_heatmap.png'):
            st.image('reports/figures/correlation_heatmap.png')
            
            st.markdown("""
            #### Interpreting the Correlation Matrix
            
            The correlation matrix shows the strength of relationships between pairs of features:
            - **Values close to 1**: Strong positive correlation (one increases, the other increases)
            - **Values close to -1**: Strong negative correlation (one increases, the other decreases)
            - **Values close to 0**: Little to no correlation
            
            The lower triangle is shown to avoid redundancy. The diagonal always equals 1 
            (each feature perfectly correlates with itself).
            """)
        else:
            st.info("Correlation matrix visualization will appear after processing data. Please run the data processing to generate this visualization.")
            if st.button("Generate Correlation Matrix"):
                process_data(DEFAULT_DATA_PATH)
                st.experimental_rerun()
        st.markdown('</div>', unsafe_allow_html=True)
    
    with eda_tabs[3]:  # Feature Analysis
        st.subheader("Churn Rate by Feature")
        
        st.markdown('<div class="plot-container">', unsafe_allow_html=True)
        if os.path.exists('reports/figures/churn_by_feature.png'):
            st.image('reports/figures/churn_by_feature.png')
            
            st.markdown("""
            #### Key Insights from Feature Analysis
            
            - **Higher churn rates** appear to correlate with specific feature ranges
            - Customers with extreme values (high or low) in certain features show different churn behaviors
            - These patterns can help identify high-risk customer segments for targeted retention strategies
            """)
        else:
            st.info("Feature analysis visualizations will appear after processing data. Please run the data processing to generate these visualizations.")
            if st.button("Generate Feature Analysis"):
                process_data(DEFAULT_DATA_PATH)
                st.experimental_rerun()
        st.markdown('</div>', unsafe_allow_html=True)

elif page == "Model Performance":
    st.title("Model Performance")
    
    if metrics is None:
        st.error("No metrics available. Please process the data.")
        if st.button("Process Data Now"):
            process_data(DEFAULT_DATA_PATH)
            st.experimental_rerun()
    else:
        # Add best model metrics at the top
        if 'best_model' in metrics and metrics['best_model'] in metrics['metrics']:
            st.subheader(f"Best Model: {metrics['best_model'].replace('_', ' ').title()}")
            
            best_model = metrics['best_model']
            best_metrics = metrics['metrics'][best_model]
            
            # Display in columns
            col1, col2, col3, col4, col5 = st.columns(5)
            
            with col1:
                st.metric("Accuracy", f"{best_metrics.get('accuracy', 0):.1%}")
            
            with col2:
                st.metric("Precision", f"{best_metrics.get('precision', 0):.1%}")
            
            with col3:
                st.metric("Recall", f"{best_metrics.get('recall', 0):.1%}")
            
            with col4:
                st.metric("F1-Score", f"{best_metrics.get('f1', 0):.1%}")
            
            with col5:
                st.metric("ROC AUC", f"{best_metrics.get('roc_auc', 0):.3f}")
            
            # Add interpretation of metrics
            st.markdown(f"""
            **Interpretation:**
            * **Accuracy:** {best_metrics.get('accuracy', 0):.1%} of all predictions are correct
            * **Precision:** When the model predicts a customer will churn, it's right {best_metrics.get('precision', 0):.1%} of the time
            * **Recall:** The model correctly identifies {best_metrics.get('recall', 0):.1%} of all customers who actually churn
            * **F1-Score:** Harmonic mean of precision and recall at {best_metrics.get('f1', 0):.1%}
            * **ROC AUC:** {best_metrics.get('roc_auc', 0):.3f} indicates excellent discriminative ability
            """)
        
        # Model Comparison section
        st.subheader("Model Comparison")
        
        # Create a DataFrame for the model comparison
        model_data = []
        
        for model_name, model_metrics in metrics['metrics'].items():
            if all(k in model_metrics for k in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']):
                model_data.append({
                    'Model': model_name.replace('_', ' ').title(),
                    'Accuracy': model_metrics['accuracy'],
                    'Precision': model_metrics['precision'],
                    'Recall': model_metrics['recall'],
                    'F1-Score': model_metrics['f1'],
                    'ROC AUC': model_metrics['roc_auc']
                })
        
        # Sort by ROC AUC (best to worst)
        model_data = sorted(model_data, key=lambda x: x['ROC AUC'], reverse=True)
        
        # Create DataFrame and display
        if model_data:
            model_df = pd.DataFrame(model_data)
            st.dataframe(model_df, use_container_width=True)
            
            # Add model ordering info
            st.info(f"Models ordered from best to worst performance based on ROC AUC score")
            
            # Check for ROC Curve and PR Curve visualizations
            curve_files = [
                ('reports/figures/roc_curve.png', 'ROC Curves'),
                ('reports/figures/pr_curve.png', 'Precision-Recall Curves'),
                ('reports/figures/confusion_matrix.png', 'Confusion Matrix')
            ]
            
            for file_path, title in curve_files:
                if os.path.exists(file_path):
                    st.subheader(title)
                    st.image(file_path)
        else:
            st.error("No valid model metrics available")

elif page == "Feature Importance":
    st.title("Feature Importance")
    
    if importance_df is None:
        st.warning("No feature importance data available. Please wait while data is processed...")
        process_data(DEFAULT_DATA_PATH)
        st.experimental_rerun()
    else:
        st.subheader("Top Features Driving Churn")
        
        # Show feature importance table
        st.dataframe(importance_df)
        
        # Plot top 10 features
        top_n = min(10, len(importance_df))
        top_features = importance_df.head(top_n)
        
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.barh(top_features['Feature'][::-1], top_features['Importance'][::-1])
        ax.set_xlabel('Importance')
        ax.set_title(f'Top {top_n} Features by Importance')
        st.pyplot(fig)

elif page == "Prediction Explanations":
    st.title("Individual Prediction Explanations")
    
    if explanations is None:
        st.warning("No explanation samples available. Please wait while data is processed...")
        process_data(DEFAULT_DATA_PATH)
        st.experimental_rerun()
    else:
        st.subheader("Sample Customer Explanations")
        
        # Let user select a sample
        sample_ids = [ex['sample_id'] for ex in explanations]
        selected_id = st.selectbox("Select a sample customer", sample_ids)
        
        # Find selected explanation
        selected_exp = next((ex for ex in explanations if ex['sample_id'] == selected_id), None)
        
        if selected_exp:
            # Display prediction vs actual
            col1, col2 = st.columns(2)
            col1.metric("Predicted Churn Probability", f"{selected_exp['prediction']:.1%}")
            col2.metric("Actual Outcome", "Churned" if selected_exp['actual'] > 0.5 else "Stayed")
            
            # Display feature contributions
            st.subheader("Key Factors Influencing This Prediction")
            
            contribution_data = []
            for contrib in selected_exp['feature_contributions']:
                contribution_data.append({
                    "Feature": contrib['feature'],
                    "Value": contrib['value'],
                    "Impact": "â†‘ Increases Risk" if contrib.get('contribution', 0) > 0 else "â†“ Decreases Risk"
                })
            
            st.table(pd.DataFrame(contribution_data))

elif page == "System Logs":
    st.title("System Logs")
    
    st.info("This page shows the most recent log entries to help you understand what's happening in the system.")
    
    # Add auto-refresh checkbox
    auto_refresh = st.checkbox("Auto-refresh logs", value=True)
    
    # Add the logs viewer
    with st.container():
        st.subheader("Recent Log Entries")
        
        # Make the logs area expandable with a fixed height
        with st.expander("View Logs", expanded=True):
            log_entries = read_logs(50)  # Get last 50 lines
            log_text = "".join(log_entries)
            st.code(log_text)
        
        # Add a manual refresh button
        if st.button("Refresh Logs"):
            st.experimental_rerun()
        
    # Auto refresh using JavaScript if selected
    if auto_refresh:
        st.markdown("""
        <script>
            setTimeout(function(){
                window.location.reload();
            }, 10000);  // Refresh every 10 seconds
        </script>
        """, unsafe_allow_html=True)

def generate_eda_visualizations(df, target_column):
    """Generate all EDA visualizations from real data and save them"""
    logger.info("Generating EDA visualizations from real data")
    
    # Create figures directory if it doesn't exist
    os.makedirs('reports/figures', exist_ok=True)
    
    # 1. Generate correlation heatmap
    plt.figure(figsize=(12, 10))
    
    # Select a subset of columns if there are too many
    if df.shape[1] > 15:
        # Get correlation with target
        corr_with_target = df.corr()[target_column].abs().sort_values(ascending=False)
        top_features = corr_with_target.head(14).index.tolist()
        if target_column not in top_features:
            top_features.append(target_column)
        corr_df = df[top_features]
    else:
        corr_df = df
    
    # Calculate correlation matrix
    corr_matrix = corr_df.corr()
    
    # Create mask for upper triangle
    mask = np.triu(np.ones_like(corr_matrix))
    
    # Generate beautiful heatmap
    sns.set(style="white")
    cmap = sns.diverging_palette(220, 10, as_cmap=True)
    
    # Draw heatmap with mask and proper aspect ratio
    sns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=.3, center=0,
              square=True, linewidths=.5, cbar_kws={"shrink": .5}, 
              annot=True, fmt=".2f")
    
    plt.title('Feature Correlation Matrix (Lower Triangle)', fontsize=16)
    plt.tight_layout()
    plt.savefig('reports/figures/correlation_heatmap.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. Generate feature distributions
    # Select numeric columns
    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
    if target_column in numeric_cols:
        numeric_cols.remove(target_column)
    
    # Limit to top 8 most correlated features
    if len(numeric_cols) > 8:
        corr_with_target = df[numeric_cols].corrwith(df[target_column]).abs().sort_values(ascending=False)
        numeric_cols = corr_with_target.head(8).index.tolist()
    
    # Create distribution grid
    rows = math.ceil(len(numeric_cols) / 2)
    fig, axes = plt.subplots(rows, 2, figsize=(14, rows*4))
    axes = axes.flatten()
    
    for i, col in enumerate(numeric_cols):
        if i < len(axes):
            # Get churned and non-churned data
            churned = df[df[target_column] == 1][col]
            retained = df[df[target_column] == 0][col]
            
            # Create KDE plot
            sns.kdeplot(retained, ax=axes[i], label='Retained', fill=True, alpha=0.5, color='#1E88E5')
            sns.kdeplot(churned, ax=axes[i], label='Churned', fill=True, alpha=0.5, color='#FFC107')
            
            # Add histogram underneath
            sns.histplot(retained, ax=axes[i], color='#1E88E5', alpha=0.3, stat="density", linewidth=0)
            sns.histplot(churned, ax=axes[i], color='#FFC107', alpha=0.3, stat="density", linewidth=0)
            
            axes[i].set_title(f'Distribution of {col}', fontsize=14)
            axes[i].set_xlabel(col, fontsize=12)
            axes[i].set_ylabel('Density', fontsize=12)
            axes[i].legend(loc='upper right')
            axes[i].grid(alpha=0.3)
    
    # Hide any unused axes
    for j in range(i+1, len(axes)):
        axes[j].set_visible(False)
    
    plt.tight_layout()
    plt.savefig('reports/figures/feature_distributions.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. Generate churn by feature visualizations
    # For numeric features: bin them and show churn rate
    # For categorical features: show churn rate by category
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    if target_column in categorical_cols:
        categorical_cols.remove(target_column)
    
    # If no categorical columns, use binned numeric columns
    if len(categorical_cols) < 2 and len(numeric_cols) > 0:
        # Create bins for top numeric features
        top_numeric = numeric_cols[:2]
        categorical_cols = []
        
        for col in top_numeric:
            # Create 4-5 bins
            df[f'{col}_binned'] = pd.qcut(df[col], q=4, duplicates='drop')
            categorical_cols.append(f'{col}_binned')
    
    # Select top categorical features (up to 2)
    top_categorical = categorical_cols[:2] if len(categorical_cols) > 0 else []
    
    if len(top_categorical) > 0:
        fig, axes = plt.subplots(1, len(top_categorical), figsize=(14, 6))
        if len(top_categorical) == 1:
            axes = [axes]
        
        for i, col in enumerate(top_categorical):
            # Calculate churn rate by category
            churn_by_category = df.groupby(col)[target_column].mean().sort_values(ascending=False)
            
            # Create bar plot
            sns.barplot(x=churn_by_category.index, y=churn_by_category.values, ax=axes[i], 
                       palette=sns.color_palette("viridis", len(churn_by_category)))
            
            # Add value labels
            for j, v in enumerate(churn_by_category.values):
                axes[i].text(j, v + 0.02, f'{v:.1%}', ha='center', va='bottom', fontsize=10)
            
            # Style the chart
            axes[i].set_title(f'Churn Rate by {col}', fontsize=14)
            axes[i].set_ylabel('Churn Rate', fontsize=12)
            axes[i].set_xlabel(col, fontsize=12)
            axes[i].set_ylim(0, max(churn_by_category.values) * 1.2)
            axes[i].grid(axis='y', alpha=0.3)
            
            # Rotate x labels if needed
            if len(churn_by_category) > 4:
                plt.setp(axes[i].get_xticklabels(), rotation=45, ha='right')
        
        plt.tight_layout()
        plt.savefig('reports/figures/churn_by_categorical.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    # 4. Generate churn by tenure/usage visualization
    # Find columns related to tenure or usage
    tenure_cols = [col for col in numeric_cols if any(term in col.lower() for term in ['tenure', 'month', 'day', 'year', 'time'])]
    
    if tenure_cols:
        tenure_col = tenure_cols[0]  # Use the first found tenure column
        
        # Create tenure bins
        tenure_bins = pd.qcut(df[tenure_col], q=5, duplicates='drop')
        churn_by_tenure = df.groupby(tenure_bins)[target_column].mean()
        
        # Create bar plot
        plt.figure(figsize=(10, 6))
        bars = plt.bar(range(len(churn_by_tenure)), churn_by_tenure.values, 
                     color=plt.cm.viridis(np.linspace(0, 0.8, len(churn_by_tenure))))
        
        # Add value labels
        for i, v in enumerate(churn_by_tenure.values):
            plt.text(i, v + 0.02, f'{v:.1%}', ha='center', va='bottom', fontsize=11)
        
        # Style the chart
        plt.title(f'Churn Rate by {tenure_col}', fontsize=16)
        plt.ylabel('Churn Rate', fontsize=14)
        plt.xlabel(tenure_col, fontsize=14)
        plt.xticks(range(len(churn_by_tenure)), [str(x) for x in churn_by_tenure.index.categories], rotation=45, ha='right')
        plt.ylim(0, max(churn_by_tenure.values) * 1.2)
        plt.grid(axis='y', alpha=0.3)
        plt.tight_layout()
        
        plt.savefig('reports/figures/churn_by_tenure.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    logger.info("EDA visualizations generated successfully")

if __name__ == "__main__":
    # This allows the script to be run directly
    pass  # Main execution handled by Streamlit 